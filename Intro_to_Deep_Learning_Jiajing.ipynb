{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning\n",
    "\n",
    "#####  Jiajing Chen     \n",
    "\n",
    "##### June, 29, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tensors and NumPy\n",
    "Tensors are containers for numbers and a generalization of matrices to an arbitrary number of dimensions (or axes). In general, all current machine-learning systems use tensors as their basic data structure. NumPy is a Python module designed for scientific computation and has several very useful features. NumPy arrays are actually tensors. NumPy also provides many useful tools to help you perform linear algebra, generate random numbers, and much much more. More info about NumPy here.\n",
    "\n",
    "NumPy arrays are an additional data type provided by NumPy and they are used for representing vectors and matrices. Unlike dynamically growing Python lists, NumPy arrays have a size that is fixed when they are constructed. Elements of NumPy arrays are also all of the same data type (by default floating point numbers), leading to more efficient and simpler code than using Python's standard data types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scalars (0D tensors)\n",
    "A tensor that contains only one number is a scalar tensor, or 0-dimensional tensor, or 0D tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "x = np.array(9)\n",
    "print (x.ndim)\n",
    "print (x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectors (1D tensors)\n",
    "A list or array of numbers is called a vector, or 1D tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3, 4])\n",
    "print (x.ndim)\n",
    "print (x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrices (2D tensors)\n",
    "An array of vectors is a matrix, or 2D tensor. The entries from the first axis are called rows and the entries from the second axis are called columns. In the following example, [1, 2, 3, 4, 5] is the first row and [1, 6, 11] is the first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(3, 5)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3, 4, 5],\n",
    "             [6, 7, 8, 9, 10],\n",
    "             [11, 12, 13, 14, 15]])\n",
    "print (x.ndim)\n",
    "print (x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3D tensors and higher-dimensional tensors\n",
    "If you pack multiple matrices into a new array, you obtain a 3D tensor, which you can visually interpret as a cube of numbers. An example is below. By packing 3D tensors into an array you can create a 4D tensor, and so on. In general, deep learning only involves 0D to 4D tensors, although video data requires 5D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(3, 3, 5)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[[1, 2, 3, 4, 5],\n",
    "             [6, 7, 8, 9, 10],\n",
    "             [11, 12, 13, 14, 15]],\n",
    "             [[1, 2, 3, 4, 5],\n",
    "             [6, 7, 8, 9, 10],\n",
    "             [11, 12, 13, 14, 15]],\n",
    "             [[1, 2, 3, 4, 5],\n",
    "             [6, 7, 8, 9, 10],\n",
    "             [11, 12, 13, 14, 15]]])\n",
    "print (x.ndim)\n",
    "print (x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key attributes of tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Number of axes (rank) - ndim in python.\n",
    "- Shape - A tuple of integers that describes how many dimensions the tensor has along each axis. In the previous example, the tensor has shape (3, 3, 5). There are 3 matrices, each with 3 rows and 5 columns. Note that a vector has shape (n, ) where n is the length of the vector, whereas a scalar has an empty shape, ().\n",
    "- Data type - dtype in python. This is the type of the data contained in the tensor. For example, a tensor's type could be float32, uint8, float64 and so on. Note that string tensors don't exist in Numpy (or in most other libraries)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting\n",
    "Broadcasting allows element-wise operations between two tensors with different shapes (when possible). The smaller tensor will be broadcasted to match the shape of the larger tensor. This happens in 2 steps:\n",
    "- Axes (called broadcast axes are added to the smaller tensor to match the ndim of the larger tensor.\n",
    "- The smaller tensor is repeated alongside these new axes to match the full shape of the larger tensor.\n",
    "As an example, consider X with shape (32, 10) and y with shape (10, ). First, we add an empty first axis to y, whose shape becomes (1, 10). Then, we repeat y 32 times alongside this new axis, so that we end up with a tensor Y with shape (32, 10) where Y[i, :] == y for i in range(0, 32). Now we can easily perform different operations, like addition, to X and Y.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 3, 32, 10)\n",
      "(24, 3, 32, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X = np.random.random((24, 3, 32, 10))  # A random tensor with shape (64, 3, 32, 10)\n",
    "y = np.random.random((32, 10))         # A random tensor with shape (32, 10)\n",
    "\n",
    "Z = np.maximum(X, y)  # The ouput Z has shape (64, 3, 32, 10) like X\n",
    "print (Z.shape)\n",
    "K = X + y\n",
    "print (K.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Dot\n",
    "The dot operation, or tensor product is the most common and morst useful tensor operation. Don't confuse it with an element-wise product - it combines entries in the input vectors. In python, the dot operator is used: z = np.dot(x,y). Note that for matrices, you can only take the dot product if x.shape[1] == y.shape[0]. The result is a matrix with shape x.shape[0] == y.shape[1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Reshaping\n",
    "Reshaping a tensor means rearranging its rows and columns to match a target shape. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0., 1.],\n",
    "              [2., 3.],\n",
    "              [4., 5.]])\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [5.]]\n"
     ]
    }
   ],
   "source": [
    "x = x.reshape((6, 1))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 2.]\n",
      " [3. 4. 5.]]\n"
     ]
    }
   ],
   "source": [
    "x = x.reshape((2, 3))\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 300)\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros((300, 20))  # Creates an all-zeros matrix of shape (300,20)\n",
    "x = np.transpose(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example - MNIST Data\n",
    "The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. Let's play around with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(60000, 28, 28)\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "print(train_images.ndim)  # Number of axes or dimensions\n",
    "print(train_images.shape) # Tensor Shape\n",
    "print(train_images.dtype) # Data type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a 3D tensor of 60,000 matrices of 28 x 28 integers. Each matrix is a greyscale image with coefficients between 0 and 255. Let's look at a specific digit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDOUAbQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x128646198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "digit = train_images[5] # Display the 6th digit in the training sample\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(digit, cmap = plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0  13  25 100 122   7   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  33 151 208 252 252 252 146   0   0   0]\n",
      " [  0   0   0   0   0   0  40 152 244 252 253 224 211 252 232  40   0   0]\n",
      " [  0   0   0   0  15 152 239 252 252 252 216  31  37 252 252  60   0   0]\n",
      " [  0   0   0   0  96 252 252 252 252 217  29   0  37 252 252  60   0   0]\n",
      " [  0   0   0   0 181 252 252 220 167  30   0   0  77 252 252  60   0   0]\n",
      " [  0   0   0   0  26 128  58  22   0   0   0   0 100 252 252  60   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 157 252 252  60   0   0]\n",
      " [  0   0   0   0   0   0   0   0 110 121 122 121 202 252 194   3   0   0]\n",
      " [  0   0   0   0   0  10  53 179 253 253 255 253 253 228  35   0   0   0]\n",
      " [  0   0   0   5  54 227 252 243 228 170 242 252 252 231 117   6   0   0]\n",
      " [  0   0   6  78 252 252 125  59   0  18 208 252 252 252 252  87   7   0]\n",
      " [  0   5 135 252 252 180  16   0  21 203 253 247 129 173 252 252 184  66]\n",
      " [  3 136 252 241 106  17   0  53 200 252 216  65   0  14  72 163 241 252]\n",
      " [105 252 242  88  18  73 170 244 252 126  29   0   0   0   0   0  89 180]\n",
      " [231 252 245 205 216 252 252 252 124   3   0   0   0   0   0   0   0   0]\n",
      " [207 252 252 252 252 178 116  36   4   0   0   0   0   0   0   0   0   0]\n",
      " [ 13  93 143 121  23   6   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print (digit[5:25,5:23])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By selecting a specific digit, we are selecting a specific element in a tensor, which is known as tensor slicing. The following code shows 3 different ways of selecting digits #10 to #100, each giving you the same object. The commas inside the brackets separate the different axes. In general, the first axis (axis 0 because indexing starts at 0 in python) will be the samples axis (sometimes called the samples dimension). In the MNIST data set, samples are images of digits. Including a : means use all elements in this axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print (train_labels[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 28, 28)\n",
      "(90, 28, 28)\n",
      "(90, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "my_slice = train_images[10:100]\n",
    "print(my_slice.shape)\n",
    "\n",
    "my_slice = train_images[10:100, :, :]       # Equivalent to the previous example\n",
    "print(my_slice.shape)\n",
    "\n",
    "my_slice = train_images[10:100, 0:28, 0:28] # Also equivalent to the previous example\n",
    "print(my_slice.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Examples of CNN on MNIST dataset\n",
    "\n",
    "First, let's take a practical look at a very simple convnet example. We will use our convnet to classify MNIST digits, a task that you've already been \n",
    "through in Chapter 2, using a densely-connected network (our test accuracy then was 97.8%). \n",
    "\n",
    "The 6 lines of code below show you what a basic convnet looks like. It's a stack of `Conv2D` and `MaxPooling2D` layers. We'll see in a \n",
    "minute what they do concretely.\n",
    "Importantly, a convnet takes as input tensors of shape `(image_height, image_width, image_channels)` (not including the batch dimension). \n",
    "In our case, we will configure our convnet to process inputs of size `(28, 28, 1)`, which is the format of MNIST images. We do this via \n",
    "passing the argument `input_shape=(28, 28, 1)` to our first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You can see above that the output of every `Conv2D` and `MaxPooling2D` layer is a 3D tensor of shape `(height, width, channels)`. The width \n",
    "and height dimensions tend to shrink as we go deeper in the network. The number of channels is controlled by the first argument passed to \n",
    "the `Conv2D` layers (e.g. 32 or 64).\n",
    "\n",
    "The next step would be to feed our last output tensor (of shape `(3, 3, 64)`) into a densely-connected classifier network like those you are \n",
    "already familiar with: a stack of `Dense` layers. These classifiers process vectors, which are 1D, whereas our current output is a 3D tensor. \n",
    "So first, we will have to flatten our 3D outputs to 1D, and then add a few `Dense` layers on top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to do 10-class classification, so we use a final layer with 10 outputs and a softmax (also known as multinomial classification) activation. Now here's what our network \n",
    "looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our `(3, 3, 64)` outputs were flattened into vectors of shape `(576,)`, before going through two `Dense` layers.\n",
    "\n",
    "Now, let's train our convnet on the MNIST digits. We will reuse a lot of the code we have already covered in the MNIST example from Chapter \n",
    "2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 35s 588us/step - loss: 0.1790 - acc: 0.9438\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 32s 537us/step - loss: 0.0494 - acc: 0.9844\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 32s 541us/step - loss: 0.0346 - acc: 0.9891\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 32s 537us/step - loss: 0.0260 - acc: 0.9922\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 32s 535us/step - loss: 0.0203 - acc: 0.9938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12c4173c8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the model on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 186us/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9822"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah! We got an accuracy of 0.99!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick look at a pre-trained convnet: VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_base = VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(150, 150, 3))\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the detail of the architecture of the VGG16 convolutional base: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example of RNN on IMDB dataset\n",
    "### The IMDB dataset\n",
    "\n",
    "\n",
    "We'll be working with \"IMDB dataset\", a set of 50,000 highly-polarized reviews from the Internet Movie Database. They are split into 25,000 \n",
    "reviews for training and 25,000 reviews for testing, each set consisting in 50% negative and 50% positive reviews.\n",
    "\n",
    "Why do we have these two separate training and test sets? You should never test a machine learning model on the same data that you used to \n",
    "train it! Just because a model performs well on its training data doesn't mean that it will perform well on data it has never seen, and \n",
    "what you actually care about is your model's performance on new data (since you already know the labels of your training data -- obviously \n",
    "you don't need your model to predict those). For instance, it is possible that your model could end up merely _memorizing_ a mapping between \n",
    "your training samples and their targets -- which would be completely useless for the task of predicting targets for data never seen before. \n",
    "We will go over this point in much more detail in future lectures.\n",
    "\n",
    "Just like the MNIST dataset, the IMDB dataset comes packaged with Keras. It has already been preprocessed: the reviews (sequences of words) \n",
    "have been turned into sequences of integers, where each integer stands for a specific word in a dictionary.\n",
    "\n",
    "The following code will load the dataset (when you run it for the first time, about 80MB of data will be downloaded to your machine):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument `num_words=10000` means that we will only keep the top 10,000 most frequently occurring words in the training data. Rare words \n",
    "will be discarded. This allows us to work with vector data of manageable size.\n",
    "\n",
    "The variables `train_data` and `test_data` are lists of reviews, each review being a list of word indices (encoding a sequence of words). \n",
    "`train_labels` and `test_labels` are lists of 0s and 1s, where 0 stands for \"negative\" and 1 stands for \"positive\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 2,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "We cannot feed lists of integers into a neural network. We have to turn our lists into tensors. There are two ways we could do that:\n",
    "\n",
    "1. We could pad our lists so that they all have the same length, and turn them into an integer tensor of shape `(samples, word_indices)`, \n",
    "then use as first layer in our network a layer capable of handling such integer tensors (the `Embedding` layer, which we will cover in \n",
    "detail later in the course).\n",
    "2. We could one-hot-encode our lists to turn them into vectors of 0s and 1s. Concretely, this would mean for instance turning the sequence \n",
    "`[3, 5]` into a 10,000-dimensional vector that would be all-zeros except for indices 3 and 5, which would be ones. Then we could use as \n",
    "first layer in our network a `Dense` layer, capable of handling floating point vector data.\n",
    "\n",
    "We will go with the latter solution. Let's vectorize our data, which we will do manually for maximum clarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
    "    return results\n",
    "\n",
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# Our vectorized test data\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what our samples look like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our vectorized labels\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our data is ready to be fed into a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our network\n",
    "\n",
    "\n",
    "Our input data is simply vectors, and our labels are scalars (1s and 0s): this is the easiest setup you will ever encounter. A type of \n",
    "network that performs well on such a problem would be a simple stack of fully-connected (`Dense`) layers with `relu` activations: `Dense(16, activation='relu')`\n",
    "\n",
    "The argument being passed to each `Dense` layer (16) is the number of \"hidden units\" of the layer. What's a hidden unit? It's a dimension \n",
    "in the representation space of the layer. You may remember from the previous chapter that each such `Dense` layer with a `relu` activation implements \n",
    "the following chain of tensor operations:\n",
    "\n",
    "`output = relu(dot(W, input) + b)`\n",
    "\n",
    "Having 16 hidden units means that the weight matrix `W` will have shape `(input_dimension, 16)`, i.e. the dot product with `W` will project the \n",
    "input data onto a 16-dimensional representation space (and then we would add the bias vector `b` and apply the `relu` operation). You can \n",
    "intuitively understand the dimensionality of your representation space as \"how much freedom you are allowing the network to have when \n",
    "learning internal representations\". Having more hidden units (a higher-dimensional representation space) allows your network to learn more \n",
    "complex representations, but it makes your network more computationally expensive and may lead to learning unwanted patterns (patterns that \n",
    "will improve performance on the training data but not on the test data).\n",
    "\n",
    "There are two key architecture decisions to be made about such stack of dense layers:\n",
    "\n",
    "* How many layers to use.\n",
    "* How many \"hidden units\" to chose for each layer.\n",
    "\n",
    "In the next few lectures, you will learn formal principles to guide you in making these choices. \n",
    "For the time being, you will have to trust us with the following architecture choice: \n",
    "two intermediate layers with 16 hidden units each, \n",
    "and a third layer which will output the scalar prediction regarding the sentiment of the current review. \n",
    "The intermediate layers will use `relu` as their \"activation function\", \n",
    "and the final layer will use a sigmoid activation so as to output a probability \n",
    "(a score between 0 and 1, indicating how likely the sample is to have the target \"1\", i.e. how likely the review is to be positive). \n",
    "A `relu` (rectified linear unit) is a function meant to zero-out negative values, \n",
    "while a sigmoid \"squashes\" arbitrary values into the `[0, 1]` interval, thus outputting something that can be interpreted as a probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import losses\n",
    "from keras import metrics\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss=losses.binary_crossentropy,\n",
    "              metrics=[metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set validation set\n",
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train our model for 20 epochs (20 iterations over all samples in the `x_train` and `y_train` tensors), in mini-batches of 512 \n",
    "samples. At this same time we will monitor loss and accuracy on the 10,000 samples that we set apart. This is done by passing the \n",
    "validation data as the `validation_data` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 2s 136us/step - loss: 0.5084 - binary_accuracy: 0.7813 - val_loss: 0.3798 - val_binary_accuracy: 0.8683\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 1s 95us/step - loss: 0.3005 - binary_accuracy: 0.9046 - val_loss: 0.3004 - val_binary_accuracy: 0.8897\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 1s 95us/step - loss: 0.2179 - binary_accuracy: 0.9287 - val_loss: 0.3086 - val_binary_accuracy: 0.8712\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 1s 94us/step - loss: 0.1750 - binary_accuracy: 0.9437 - val_loss: 0.2840 - val_binary_accuracy: 0.8832\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 1s 95us/step - loss: 0.1427 - binary_accuracy: 0.9543 - val_loss: 0.2841 - val_binary_accuracy: 0.8872\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 1s 95us/step - loss: 0.1150 - binary_accuracy: 0.9651 - val_loss: 0.3166 - val_binary_accuracy: 0.8774\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 1s 95us/step - loss: 0.0980 - binary_accuracy: 0.9708 - val_loss: 0.3127 - val_binary_accuracy: 0.8844\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 1s 94us/step - loss: 0.0807 - binary_accuracy: 0.9764 - val_loss: 0.3860 - val_binary_accuracy: 0.8649\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 1s 95us/step - loss: 0.0661 - binary_accuracy: 0.9821 - val_loss: 0.3636 - val_binary_accuracy: 0.8780\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 1s 95us/step - loss: 0.0565 - binary_accuracy: 0.9852 - val_loss: 0.3842 - val_binary_accuracy: 0.8801\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 1s 96us/step - loss: 0.0430 - binary_accuracy: 0.9904 - val_loss: 0.4133 - val_binary_accuracy: 0.8782\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 1s 95us/step - loss: 0.0376 - binary_accuracy: 0.9923 - val_loss: 0.4594 - val_binary_accuracy: 0.8674\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 1s 96us/step - loss: 0.0301 - binary_accuracy: 0.9929 - val_loss: 0.4707 - val_binary_accuracy: 0.8733\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 2s 104us/step - loss: 0.0246 - binary_accuracy: 0.9947 - val_loss: 0.5032 - val_binary_accuracy: 0.8727\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 2s 119us/step - loss: 0.0183 - binary_accuracy: 0.9973 - val_loss: 0.5310 - val_binary_accuracy: 0.8706\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 2s 112us/step - loss: 0.0180 - binary_accuracy: 0.9960 - val_loss: 0.5638 - val_binary_accuracy: 0.8700\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 2s 124us/step - loss: 0.0105 - binary_accuracy: 0.9993 - val_loss: 0.6036 - val_binary_accuracy: 0.8665\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 2s 110us/step - loss: 0.0133 - binary_accuracy: 0.9971 - val_loss: 0.6345 - val_binary_accuracy: 0.8675\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 2s 112us/step - loss: 0.0063 - binary_accuracy: 0.9995 - val_loss: 0.7281 - val_binary_accuracy: 0.8561\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 1s 98us/step - loss: 0.0049 - binary_accuracy: 0.9998 - val_loss: 0.7185 - val_binary_accuracy: 0.8612\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_binary_accuracy', 'loss', 'binary_accuracy'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14e29d358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "val_binary_accuracy = history.history['val_binary_accuracy']\n",
    "binary_accuracy = history.history['binary_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(binary_accuracy) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x153766908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # clear figure\n",
    "acc_values = history_dict['binary_accuracy']\n",
    "val_acc_values = history_dict['val_binary_accuracy']\n",
    "\n",
    "plt.plot(epochs, binary_accuracy, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_binary_accuracy, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The dots are the training loss and accuracy, while the solid lines are the validation loss and accuracy. Note that your own results may vary \n",
    "slightly due to a different random initialization of your network.\n",
    "\n",
    "As you can see, the training loss decreases with every epoch and the training accuracy increases with every epoch. That's what you would \n",
    "expect when running gradient descent optimization -- the quantity you are trying to minimize should get lower with every iteration. But that \n",
    "isn't the case for the validation loss and accuracy: they seem to peak at the fourth epoch. This is an example of what we were warning \n",
    "against earlier: a model that performs better on the training data isn't necessarily a model that will do better on data it has never seen \n",
    "before. In precise terms, what you are seeing is \"overfitting\": after the second epoch, we are over-optimizing on the training data, and we \n",
    "ended up learning representations that are specific to the training data and do not generalize to data outside of the training set.\n",
    "\n",
    "In this case, to prevent overfitting, we could simply stop training after three epochs. In general, there is a range of techniques you can \n",
    "leverage to mitigate overfitting, which we will cover in the next chapter.\n",
    "\n",
    "Let's train a new network from scratch for four epochs, then evaluate it on our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 2s 74us/step - loss: 0.4750 - acc: 0.8216\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 2s 66us/step - loss: 0.2652 - acc: 0.9096\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 2s 66us/step - loss: 0.1983 - acc: 0.9302\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 2s 67us/step - loss: 0.1671 - acc: 0.9408\n",
      "25000/25000 [==============================] - 2s 77us/step\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.32342941537857056, 0.87296]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's some take away from this example:\n",
    "\n",
    "* There's usually quite a bit of preprocessing you need to do on your raw data in order to be able to feed it -- as tensors -- into a neural \n",
    "network. In the case of sequences of words, they can be encoded as binary vectors -- but there are other encoding options too.\n",
    "* Stacks of `Dense` layers with `relu` activations can solve a wide range of problems (including sentiment classification), and you will \n",
    "likely use them frequently.\n",
    "* In a binary classification problem (two output classes), your network should end with a `Dense` layer with 1 unit and a `sigmoid` activation, \n",
    "i.e. the output of your network should be a scalar between 0 and 1, encoding a probability.\n",
    "* With such a scalar sigmoid output, on a binary classification problem, the loss function you should use is `binary_crossentropy`.\n",
    "* The `rmsprop` optimizer is generally a good enough choice of optimizer, whatever your problem. That's one less thing for you to worry \n",
    "about.\n",
    "* As they get better on their training data, neural networks eventually start _overfitting_ and end up obtaining increasingly worse results on data \n",
    "never-seen-before. Make sure to always monitor performance on data that is outside of the training set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "This notebook contains the code sample found in [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
